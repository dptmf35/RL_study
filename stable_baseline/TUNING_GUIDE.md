# ğŸ›ï¸ PPO í•™ìŠµ íŠœë‹ ê°€ì´ë“œ

## ğŸ“Š ë¹ ë¥¸ ì§„ë‹¨

### ë¬¸ì œë³„ í•´ê²°ì±…

| ì¦ìƒ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| Success rate 0% | íƒœìŠ¤í¬ ë„ˆë¬´ ì–´ë ¤ì›€ | â‘  goal_height ë‚®ì¶”ê¸° â‘¡ max_episode_steps ëŠ˜ë¦¬ê¸° |
| ë¡œë´‡ì´ íë¸Œì— ì•ˆ ê° | Reaching reward ë¶€ì¡± | reaching_reward ê°€ì¤‘ì¹˜ ì¦ê°€ |
| íë¸Œ ê·¼ì²˜ë§Œ ë§´ëŒê³  ì•ˆ ì¡ìŒ | Grasping reward ë¶€ì¡± | grasp_reward ê°€ì¤‘ì¹˜ ì¦ê°€ (5.0+) |
| **ì¡ê¸°ëŠ” í•˜ëŠ”ë° ì•ˆ ë“¤ì–´ì˜¬ë¦¼** â­ | **Lifting reward ë¶€ì¡±** | **lift_reward ëŒ€í­ ì¦ê°€ (8.0~15.0)** |
| ë¦¬ì›Œë“œëŠ” ë†’ì€ë° ì„±ê³µ 0% | Episode ë„ˆë¬´ ì§§ìŒ | max_episode_steps ì¦ê°€ (300+) |
| ë¦¬ì›Œë“œ ë³€ë™ì´ ì‹¬í•¨ | íƒìƒ‰ ë¶€ì¡± | ent_coef ì¦ê°€ (0.01), n_steps ì¦ê°€ |
| í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼ | ë°ì´í„° ë¶€ì¡± | num_envs ì¦ê°€ (64), n_steps ì¦ê°€ |

---

## ğŸ¯ í™˜ê²½ íŒŒë¼ë¯¸í„° (my_pick_cube_env.py)

### ë‚œì´ë„ ì¡°ì ˆ
```python
# ì¤„ 23
@register_env("MyPickCube-v0", max_episode_steps=300)

# ì¤„ 30-31
cube_half_size = 0.02      # íë¸Œ í¬ê¸°
goal_height = 0.2          # ëª©í‘œ ë†’ì´
```

**ì¡°ì • ê°€ì´ë“œ:**
- `goal_height`: 0.15 (ì‰¬ì›€) ~ 0.3 (ì–´ë ¤ì›€)
- `max_episode_steps`: 200 (ë¹ ë¦„) ~ 500 (ì¶©ë¶„í•¨)
- `cube_half_size`: 0.02 (ì ë‹¹) ~ 0.03 (ì‰¬ì›€)

### ë¦¬ì›Œë“œ ê°€ì¤‘ì¹˜ (ê°€ì¥ ì¤‘ìš”!)
```python
# ì¤„ 173-178
reward = (
    reaching_reward * 1.5    # íë¸Œë¡œ ì´ë™
    + grasp_reward * 4.0     # íë¸Œ ì¡ê¸°
    + lift_reward * 10.0     # ë“¤ì–´ì˜¬ë¦¬ê¸° â­â­â­
    + success * 15.0         # ì„±ê³µ ë³´ë„ˆìŠ¤
)
```

**ë‹¨ê³„ë³„ ê°€ì¤‘ì¹˜ ì „ëµ:**

#### Phase 1: Reaching í•™ìŠµ (ì´ˆê¸°)
```python
reaching * 3.0, grasp * 2.0, lift * 2.0, success * 5.0
```

#### Phase 2: Grasping í•™ìŠµ (ì¤‘ê¸°)
```python
reaching * 2.0, grasp * 5.0, lift * 5.0, success * 10.0
```

#### Phase 3: Lifting í•™ìŠµ (í˜„ì¬) â­
```python
reaching * 1.5, grasp * 4.0, lift * 10.0, success * 15.0
```

### Success ì¡°ê±´
```python
# ì¤„ 131
success = cube_pos[:, 2] >= self.goal_height - 0.08
```

**ì¡°ì •:**
- `-0.05`: ì—„ê²© (ì •í™•íˆ ëª©í‘œ ë†’ì´)
- `-0.08`: ê´€ëŒ€ (í˜„ì¬, ì¶”ì²œ)
- `-0.10`: ë§¤ìš° ê´€ëŒ€

---

## ğŸ¤– PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° (train_ppo_pickcube.py)

### ê¸°ë³¸ ì„¤ì •
```python
model = PPO(
    "MlpPolicy",
    train_env,
    learning_rate=3e-4,      # í•™ìŠµë¥ 
    n_steps=256,             # ì—…ë°ì´íŠ¸ë‹¹ ìŠ¤í… ìˆ˜
    batch_size=256,          # ë°°ì¹˜ í¬ê¸°
    n_epochs=10,             # ì—í­ ìˆ˜
    gamma=0.99,              # í• ì¸ìœ¨
    gae_lambda=0.95,         # GAE lambda
    clip_range=0.2,          # PPO clip
    ent_coef=0.0,           # Entropy ê³„ìˆ˜
    verbose=1,
)
```

### íŒŒë¼ë¯¸í„°ë³„ ì„¤ëª…

#### Learning Rate (í•™ìŠµë¥ )
- `1e-4`: ì•ˆì •ì , ëŠë¦¼
- `3e-4`: **ê¸°ë³¸, ì¶”ì²œ**
- `1e-3`: ë¹ ë¦„, ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ

#### N Steps (ê²½í—˜ ìˆ˜ì§‘)
- `128`: ë¹ ë¦„, ë°ì´í„° ì ìŒ
- `256`: **ê¸°ë³¸, ì¶”ì²œ**
- `512`: ëŠë¦¼, ë°ì´í„° ë§ìŒ, ë” ì•ˆì •

#### Batch Size
- `128`: ë¹ ë¦„, ë…¸ì´ì¦ˆ ë§ìŒ
- `256`: **ê¸°ë³¸, ì¶”ì²œ**
- `512`: ëŠë¦¼, ì•ˆì •ì 

#### N Epochs (í•™ìŠµ ë°˜ë³µ)
- `5`: ë¹ ë¦„, underfitting ìœ„í—˜
- `10`: **ê¸°ë³¸, ì¶”ì²œ**
- `20`: ëŠë¦¼, overfitting ìœ„í—˜

#### Gamma (í• ì¸ìœ¨)
- `0.95`: ë‹¨ê¸° ë³´ìƒ ì¤‘ì‹œ
- `0.99`: **ì¥ê¸° ë³´ìƒ ì¤‘ì‹œ, ì¶”ì²œ**
- `0.999`: ë§¤ìš° ì¥ê¸°ì 

#### Entropy Coefficient (íƒìƒ‰)
- `0.0`: **íƒìƒ‰ ì•ˆ í•¨, ê¸°ë³¸**
- `0.01`: ì ë‹¹í•œ íƒìƒ‰
- `0.05`: ë§ì€ íƒìƒ‰

---

## ğŸƒ ì‹¤ì „ ì „ëµ

### Strategy 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
```python
num_envs = 16              # ì ì€ í™˜ê²½
total_timesteps = 200_000  # ì§§ì€ í•™ìŠµ
```
â†’ 10ë¶„ ë‚´ ê²°ê³¼ í™•ì¸, ë¦¬ì›Œë“œ íŠœë‹ìš©

### Strategy 2: í‘œì¤€ í•™ìŠµ (ì¶”ì²œ)
```python
num_envs = 32              
total_timesteps = 500_000  
```
â†’ 30ë¶„~1ì‹œê°„, ì¶©ë¶„í•œ í•™ìŠµ

### Strategy 3: ê³ í’ˆì§ˆ í•™ìŠµ
```python
num_envs = 64              
total_timesteps = 1_000_000
n_steps = 512
```
â†’ 2~3ì‹œê°„, ìµœê³  ì„±ëŠ¥

---

## ğŸ“ˆ í•™ìŠµ ëª¨ë‹ˆí„°ë§

### Tensorboard
```bash
tensorboard --logdir=tensorboard_logs/
```

**í™•ì¸í•  ì§€í‘œ:**
- `rollout/ep_rew_mean`: ì¦ê°€í•´ì•¼ í•¨
- `rollout/ep_len_mean`: ì´ˆê¸°ì—” ì§§ë‹¤ê°€ ì ì  ê¸¸ì–´ì§
- `train/policy_gradient_loss`: ë„ˆë¬´ í¬ë©´ learning_rate ë‚®ì¶”ê¸°
- `train/entropy_loss`: 0ì— ê°€ê¹Œìš°ë©´ íƒìƒ‰ ë¶€ì¡±

### í‰ê°€
```bash
# 50k ìŠ¤í…ë§ˆë‹¤ ìë™ í‰ê°€ (ì½œë°±)
# ë˜ëŠ” ìˆ˜ë™:
python eval_ppo_pickcube.py --model checkpoints/ppo_pickcube_200000_steps
```

---

## ğŸ¬ í˜„ì¬ "ë“¤ì–´ì˜¬ë¦¬ê¸°" ë¬¸ì œ í•´ê²° ì „ëµ

### âœ… ì´ë¯¸ ì ìš©ëœ ê²ƒ
1. `max_episode_steps`: 200 â†’ 300
2. `goal_height`: 0.3 â†’ 0.2
3. Success ì¡°ê±´: -0.05 â†’ -0.08
4. **Lift reward**: 5.0 â†’ 10.0 â­â­â­

### ğŸš€ ì§€ê¸ˆ í•  ì¼
```bash
# 1. ì²˜ìŒë¶€í„° ì¬í•™ìŠµ (ìƒˆ ë¦¬ì›Œë“œ êµ¬ì¡°)
python train_ppo_pickcube.py

# 2. í•™ìŠµ ì™„ë£Œ í›„ í‰ê°€
python eval_ppo_pickcube.py --model ppo_pickcube_final --episodes 20

# 3. ë¹„ë””ì˜¤ í™•ì¸
ls -lh eval_videos_test/
```

### ğŸ“Š ê¸°ëŒ€ ê²°ê³¼
- **ì´ì „**: Reward 58, Success 0%
- **ì˜ˆìƒ**: Reward 70+, Success 30-60%

---

## ğŸ’¡ ì¶”ê°€ íŒ

### Curriculum Learning (ë‹¨ê³„ì  í•™ìŠµ)
1. ì‰¬ìš´ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ (`goal_height=0.15`)
2. ëª¨ë¸ ì €ì¥
3. ì–´ë ¤ìš´ ì„¤ì •ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ (`goal_height=0.25`)

### ë¦¬ì›Œë“œ Shaping íŒ
- **í¬ì†Œ ë¦¬ì›Œë“œ**: ì„±ê³µ ì‹œì—ë§Œ í° ë¦¬ì›Œë“œ (ì„±ê³µ ë°©ë²• ëª…í™•í•  ë•Œ)
- **ë°€ì§‘ ë¦¬ì›Œë“œ**: ë§¤ ìŠ¤í… ì‘ì€ ë¦¬ì›Œë“œ (íƒìƒ‰ í•„ìš”í•  ë•Œ) â† **í˜„ì¬ ì‚¬ìš©**

### ë””ë²„ê¹…
```python
# ë¦¬ì›Œë“œ ê° í•­ëª©ë³„ë¡œ ì¶œë ¥
print(f"Reaching: {reaching_reward.mean():.2f}")
print(f"Grasping: {grasp_reward.sum()}/{len(grasp_reward)}")
print(f"Lifting: {lift_reward.mean():.2f}")
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²° ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë¹„ë””ì˜¤ë¡œ ë¡œë´‡ í–‰ë™ í™•ì¸
- [ ] Tensorboardë¡œ í•™ìŠµ ê³¡ì„  í™•ì¸
- [ ] ë¦¬ì›Œë“œ ê° í•­ëª© ê°€ì¤‘ì¹˜ í™•ì¸
- [ ] Episode ê¸¸ì´ ì¶©ë¶„í•œì§€ í™•ì¸
- [ ] Success ì¡°ê±´ì´ ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šì€ì§€ í™•ì¸
- [ ] ì—¬ëŸ¬ ì‹œë“œë¡œ ì¬í˜„ì„± í™•ì¸

---

**í˜„ì¬ ìµœì í™” í¬ì¸íŠ¸: Lifting rewardë¥¼ 10.0ìœ¼ë¡œ ëŒ€í­ ì¦ê°€!** ğŸš€
