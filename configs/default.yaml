# Default configuration for UR5e Pick and Place PPO training

# Environment settings
environment:
  reward_type: dense      # dense or sparse
  max_episode_steps: 500
  randomize_cube: true
  randomize_target: false

# Training settings
training:
  total_timesteps: 2000000
  n_envs: 8
  seed: 42

# PPO hyperparameters
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 256
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5

# Network architecture
network:
  policy_layers: [256, 256, 128]
  value_layers: [256, 256, 128]
  activation: relu

# Logging and checkpoints
logging:
  log_dir: logs
  model_dir: models
  save_freq: 50000
  eval_freq: 20000
  n_eval_episodes: 10
