# ë¡œë´‡ ê°•í™”í•™ìŠµ ì…ë¬¸ - 1ë‹¨ê³„: ê¸°ë³¸ í™˜ê²½
# í•„ìš”í•œ ì„¤ì¹˜: pip install gymnasium stable-baselines3 matplotlib

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback
import os

def test_basic_rl():
    """ê¸°ë³¸ ê°•í™”í•™ìŠµ í™˜ê²½ìœ¼ë¡œ ì‹œì‘"""
    print("=== 1ë‹¨ê³„: ê¸°ë³¸ CartPole í™˜ê²½ í…ŒìŠ¤íŠ¸ ===")
    
    # í™˜ê²½ ìƒì„±
    env = gym.make('CartPole-v1', render_mode='human')
    
    print(f"ê´€ì¸¡ ê³µê°„: {env.observation_space}")
    print(f"í–‰ë™ ê³µê°„: {env.action_space}")
    
    # PPO ëª¨ë¸ ìƒì„±
    model = PPO('MlpPolicy', env, verbose=1, learning_rate=0.0003)
    
    # í•™ìŠµ
    print("í•™ìŠµ ì‹œì‘...")
    model.learn(total_timesteps=20000)
    
    # í…ŒìŠ¤íŠ¸
    print("í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    obs, info = env.reset()
    total_reward = 0
    
    for i in range(500):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        
        if terminated or truncated:
            print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ - ì´ ë³´ìƒ: {total_reward}")
            break
    
    env.close()
    return model

def test_pendulum():
    """ì—°ì† ì œì–´ í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    print("\n=== 2ë‹¨ê³„: Pendulum ì—°ì† ì œì–´ í™˜ê²½ ===")
    
    # ì—°ì† ì œì–´ í™˜ê²½
    env = gym.make('Pendulum-v1', render_mode='human')
    
    print(f"ê´€ì¸¡ ê³µê°„: {env.observation_space}")
    print(f"í–‰ë™ ê³µê°„: {env.action_space}")
    
    # PPO ëª¨ë¸ (ì—°ì† í–‰ë™ ê³µê°„ìš©)
    model = PPO('MlpPolicy', env, verbose=1, learning_rate=0.001)
    
    # í•™ìŠµ
    print("Pendulum í•™ìŠµ ì‹œì‘...")
    model.learn(total_timesteps=30000)
    
    # í…ŒìŠ¤íŠ¸
    print("í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    obs, info = env.reset()
    total_reward = 0
    
    for i in range(200):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        
        if terminated or truncated:
            obs, info = env.reset()
    
    print(f"í‰ê·  ë³´ìƒ: {total_reward/200:.2f}")
    env.close()
    return model

def plot_learning_curve():
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
    print("\n=== 3ë‹¨ê³„: í•™ìŠµ ê³¡ì„  ì‹œê°í™” ===")
    
    # ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„± (ë³‘ë ¬ í•™ìŠµ)
    env = make_vec_env('CartPole-v1', n_envs=4)
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = gym.make('CartPole-v1')
    
    # ì½œë°± ì„¤ì • (í•™ìŠµ ê³¼ì • ê¸°ë¡)
    eval_callback = EvalCallback(
        eval_env, 
        best_model_save_path='./logs/',
        log_path='./logs/', 
        eval_freq=2000,
        deterministic=True, 
        render=False
    )
    
    # ëª¨ë¸ ìƒì„±
    model = PPO('MlpPolicy', env, verbose=1, tensorboard_log="./tensorboard/")
    
    # í•™ìŠµ (ì½œë°±ê³¼ í•¨ê»˜)
    model.learn(total_timesteps=50000, callback=eval_callback)
    
    print("í•™ìŠµ ì™„ë£Œ! tensorboardë¡œ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥: tensorboard --logdir ./tensorboard/")
    
    env.close()
    eval_env.close()
    return model

def compare_algorithms():
    """ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ"""
    print("\n=== 4ë‹¨ê³„: ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ===")
    
    from stable_baselines3 import A2C, DQN
    
    env = gym.make('CartPole-v1')
    algorithms = {
        'PPO': PPO('MlpPolicy', env, verbose=0),
        'A2C': A2C('MlpPolicy', env, verbose=0),
        'DQN': DQN('MlpPolicy', env, verbose=0)
    }
    
    results = {}
    
    for name, model in algorithms.items():
        print(f"{name} í•™ìŠµ ì¤‘...")
        model.learn(total_timesteps=20000)
        
        # í‰ê°€
        total_rewards = []
        for _ in range(10):
            obs, info = env.reset()
            episode_reward = 0
            
            for _ in range(500):
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                
                if terminated or truncated:
                    break
            
            total_rewards.append(episode_reward)
        
        results[name] = np.mean(total_rewards)
        print(f"{name} í‰ê·  ë³´ìƒ: {results[name]:.2f}")
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    algorithms_names = list(results.keys())
    scores = list(results.values())
    
    plt.bar(algorithms_names, scores)
    plt.title('ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ë¹„êµ')
    plt.ylabel('í‰ê·  ë³´ìƒ')
    plt.xlabel('ì•Œê³ ë¦¬ì¦˜')
    plt.show()
    
    env.close()
    return results

if __name__ == "__main__":
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./logs', exist_ok=True)
    
    print("ğŸ¤– ë¡œë´‡ ê°•í™”í•™ìŠµ ì…ë¬¸ ê³¼ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 50)
    
    # 1ë‹¨ê³„: ê¸°ë³¸ í™˜ê²½
    model1 = test_basic_rl()
    
    # 2ë‹¨ê³„: ì—°ì† ì œì–´
    model2 = test_pendulum()
    
    # 3ë‹¨ê³„: í•™ìŠµ ê³¡ì„ 
    model3 = plot_learning_curve()
    
    # 4ë‹¨ê³„: ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
    results = compare_algorithms()
    
    print("\nğŸ‰ ì…ë¬¸ ê³¼ì • ì™„ë£Œ!")
    print("ë‹¤ìŒ ë‹¨ê³„: PyBullet í™˜ê²½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")