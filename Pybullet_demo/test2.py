# ë¡œë´‡ ê°•í™”í•™ìŠµ ì…ë¬¸ - 2ë‹¨ê³„: PyBullet ë¡œë´‡ í™˜ê²½
# í•„ìš”í•œ ì„¤ì¹˜: pip install pybullet gymnasium stable-baselines3

import gymnasium as gym
import pybullet as p
import pybullet_data
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO, SAC
from stable_baselines3.common.env_util import make_vec_env
import time
import os

class SimpleRobotEnv(gym.Env):
    """ê°„ë‹¨í•œ PyBullet ë¡œë´‡ í™˜ê²½ ì§ì ‘ êµ¬í˜„"""
    
    def __init__(self, render_mode=None):
        super(SimpleRobotEnv, self).__init__()
        
        # í–‰ë™ ê³µê°„: 2ê°œ ë°”í€´ì˜ ì†ë„ (-1 ~ 1)
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(2,), dtype=np.float32
        )
        
        # ê´€ì¸¡ ê³µê°„: [x, y, orientation, x_vel, y_vel, ang_vel]
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32
        )
        
        self.render_mode = render_mode
        self.physics_client = None
        self.robot_id = None
        self.target_pos = [2, 2, 0]
        self.max_steps = 1000
        self.current_step = 0
        
    def reset(self, seed=None):
        super().reset(seed=seed)
        
        # PyBullet ì´ˆê¸°í™”
        if self.physics_client is not None:
            p.disconnect(self.physics_client)
            
        if self.render_mode == "human":
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)
            
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.8)
        
        # ë°”ë‹¥ ìƒì„±
        p.loadURDF("plane.urdf")
        
        # ê°„ë‹¨í•œ ë¡œë´‡ ìƒì„± (ìƒì + ë°”í€´)
        self.robot_id = self._create_simple_robot()
        
        # ëª©í‘œì  ì‹œê°í™”
        p.loadURDF("sphere2.urdf", self.target_pos, globalScaling=0.1)
        
        self.current_step = 0
        
        return self._get_observation(), {}
    
    def _create_simple_robot(self):
        """ê°„ë‹¨í•œ 2ë°”í€´ ë¡œë´‡ ìƒì„±"""
        # ëª¸ì²´
        body_collision = p.createCollisionShape(p.GEOM_BOX, halfExtents=[0.2, 0.1, 0.05])
        body_visual = p.createVisualShape(p.GEOM_BOX, halfExtents=[0.2, 0.1, 0.05], rgbaColor=[0, 0, 1, 1])
        
        # ë°”í€´
        wheel_collision = p.createCollisionShape(p.GEOM_CYLINDER, radius=0.05, height=0.02)
        wheel_visual = p.createVisualShape(p.GEOM_CYLINDER, radius=0.05, length=0.02, rgbaColor=[1, 0, 0, 1])
        
        # ë¡œë´‡ ì¡°ë¦½
        robot_id = p.createMultiBody(
            baseMass=1,
            baseCollisionShapeIndex=body_collision,
            baseVisualShapeIndex=body_visual,
            basePosition=[0, 0, 0.1],
            linkMasses=[0.1, 0.1],
            linkCollisionShapeIndices=[wheel_collision, wheel_collision],
            linkVisualShapeIndices=[wheel_visual, wheel_visual],
            linkPositions=[[0.1, -0.15, 0], [0.1, 0.15, 0]],
            linkOrientations=[[0, 0, 0, 1], [0, 0, 0, 1]],
            linkInertialFramePositions=[[0, 0, 0], [0, 0, 0]],
            linkInertialFrameOrientations=[[0, 0, 0, 1], [0, 0, 0, 1]],
            linkParentIndices=[0, 0],
            linkJointTypes=[p.JOINT_REVOLUTE, p.JOINT_REVOLUTE],
            linkJointAxis=[[0, 1, 0], [0, 1, 0]]
        )
        
        return robot_id
    
    def step(self, action):
        # ë°”í€´ ì†ë„ ì ìš©
        left_wheel_vel = action[0] * 10  # ìµœëŒ€ ì†ë„ 10 rad/s
        right_wheel_vel = action[1] * 10
        
        p.setJointMotorControl2(
            self.robot_id, 0, p.VELOCITY_CONTROL, targetVelocity=left_wheel_vel
        )
        p.setJointMotorControl2(
            self.robot_id, 1, p.VELOCITY_CONTROL, targetVelocity=right_wheel_vel
        )
        
        # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        p.stepSimulation()
        
        # ê´€ì¸¡ê°’ ë° ë³´ìƒ ê³„ì‚°
        obs = self._get_observation()
        reward = self._calculate_reward()
        
        self.current_step += 1
        terminated = self._is_terminated()
        truncated = self.current_step >= self.max_steps
        
        return obs, reward, terminated, truncated, {}
    
    def _get_observation(self):
        """ë¡œë´‡ì˜ í˜„ì¬ ìƒíƒœ ê´€ì¸¡"""
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        vel, ang_vel = p.getBaseVelocity(self.robot_id)
        
        # ì˜¤ì¼ëŸ¬ ê°ë„ë¡œ ë³€í™˜
        euler = p.getEulerFromQuaternion(orn)
        
        obs = np.array([
            pos[0], pos[1],  # x, y ìœ„ì¹˜
            euler[2],        # zì¶• íšŒì „ê°
            vel[0], vel[1],  # x, y ì†ë„
            ang_vel[2]       # ê°ì†ë„
        ], dtype=np.float32)
        
        return obs
    
    def _calculate_reward(self):
        """ë³´ìƒ í•¨ìˆ˜"""
        pos, _ = p.getBasePositionAndOrientation(self.robot_id)
        
        # ëª©í‘œì ê¹Œì§€ì˜ ê±°ë¦¬
        distance = np.sqrt((pos[0] - self.target_pos[0])**2 + 
                          (pos[1] - self.target_pos[1])**2)
        
        # ê±°ë¦¬ì— ë°˜ë¹„ë¡€í•˜ëŠ” ë³´ìƒ
        reward = -distance
        
        # ëª©í‘œì  ê·¼ì²˜ ë„ì°© ì‹œ í° ë³´ìƒ
        if distance < 0.2:
            reward += 100
            
        # ë§µ ë°–ìœ¼ë¡œ ë‚˜ê°€ë©´ íŒ¨ë„í‹°
        if abs(pos[0]) > 5 or abs(pos[1]) > 5:
            reward -= 50
            
        return reward
    
    def _is_terminated(self):
        """ì¢…ë£Œ ì¡°ê±´"""
        pos, _ = p.getBasePositionAndOrientation(self.robot_id)
        distance = np.sqrt((pos[0] - self.target_pos[0])**2 + 
                          (pos[1] - self.target_pos[1])**2)
        
        # ëª©í‘œì  ë„ì°© ë˜ëŠ” ë§µ ë°–ìœ¼ë¡œ ë‚˜ê°
        return distance < 0.2 or abs(pos[0]) > 5 or abs(pos[1]) > 5
    
    def render(self):
        if self.render_mode == "human":
            time.sleep(0.01)  # ì‹œê°í™” ì†ë„ ì¡°ì ˆ
    
    def close(self):
        if self.physics_client is not None:
            p.disconnect(self.physics_client)

def train_simple_robot():
    """ê°„ë‹¨í•œ ë¡œë´‡ í•™ìŠµ"""
    print("=== PyBullet ê°„ë‹¨í•œ ë¡œë´‡ í•™ìŠµ ===")
    
    # í™˜ê²½ ìƒì„±
    env = SimpleRobotEnv(render_mode=None)
    
    print(f"ê´€ì¸¡ ê³µê°„: {env.observation_space}")
    print(f"í–‰ë™ ê³µê°„: {env.action_space}")
    
    # PPO ëª¨ë¸ ìƒì„±
    model = PPO('MlpPolicy', env, verbose=1, learning_rate=0.0003)
    
    # í•™ìŠµ
    print("í•™ìŠµ ì‹œì‘...")
    model.learn(total_timesteps=50000)
    
    # ëª¨ë¸ ì €ì¥
    model.save("simple_robot_ppo")
    
    return model, env

def test_trained_robot():
    """í•™ìŠµëœ ë¡œë´‡ í…ŒìŠ¤íŠ¸"""
    print("\n=== í•™ìŠµëœ ë¡œë´‡ í…ŒìŠ¤íŠ¸ ===")
    
    # í™˜ê²½ ìƒì„± (ì‹œê°í™” ëª¨ë“œ)
    env = SimpleRobotEnv(render_mode="human")
    
    # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    try:
        model = PPO.load("simple_robot_ppo")
        print("ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        print("ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    obs, info = env.reset()
    total_reward = 0
    
    for i in range(1000):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        env.render()
        
        if terminated or truncated:
            print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ - ì´ ë³´ìƒ: {total_reward:.2f}")
            obs, info = env.reset()
            total_reward = 0
    
    env.close()

def try_builtin_envs():
    """PyBullet ë‚´ì¥ í™˜ê²½ë“¤ ì‹œë„"""
    print("\n=== PyBullet ë‚´ì¥ í™˜ê²½ í…ŒìŠ¤íŠ¸ ===")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ë“¤ ì‹œë„
    test_envs = [
        'CartPole-v1',  # ê¸°ë³¸ í™˜ê²½
        'Pendulum-v1',  # ì—°ì† ì œì–´ í™˜ê²½
    ]
    
    for env_name in test_envs:
        try:
            print(f"\n{env_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            env = gym.make(env_name, render_mode='rgb_array')
            
            # ê°„ë‹¨í•œ ëœë¤ ì •ì±… í…ŒìŠ¤íŠ¸
            obs, info = env.reset()
            total_reward = 0
            
            for _ in range(100):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                
                if terminated or truncated:
                    break
            
            print(f"{env_name} - ëœë¤ ì •ì±… ë³´ìƒ: {total_reward:.2f}")
            env.close()
            
        except Exception as e:
            print(f"{env_name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")

def advanced_robot_training():
    """ê³ ê¸‰ ë¡œë´‡ í•™ìŠµ (SAC ì•Œê³ ë¦¬ì¦˜)"""
    print("\n=== ê³ ê¸‰ ë¡œë´‡ í•™ìŠµ (SAC) ===")
    
    env = SimpleRobotEnv(render_mode="human")
    
    # SAC ëª¨ë¸ (ì—°ì† ì œì–´ì— íŠ¹í™”)
    model = SAC('MlpPolicy', env, verbose=1, learning_rate=0.0003)
    
    print("SAC í•™ìŠµ ì‹œì‘...")
    model.learn(total_timesteps=30000)
    
    # ëª¨ë¸ ì €ì¥
    model.save("simple_robot_sac")
    
    # ê°„ë‹¨í•œ ì„±ëŠ¥ í‰ê°€
    obs, info = env.reset()
    total_rewards = []
    
    for episode in range(10):
        episode_reward = 0
        obs, info = env.reset()
        
        for _ in range(1000):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
    
    print(f"SAC í‰ê·  ì„±ëŠ¥: {np.mean(total_rewards):.2f} Â± {np.std(total_rewards):.2f}")
    env.close()

if __name__ == "__main__":
    print("ğŸ¤– PyBullet ë¡œë´‡ ê°•í™”í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 50)
    
    # 1. ë‚´ì¥ í™˜ê²½ í…ŒìŠ¤íŠ¸
    try_builtin_envs()
    
    # 2. ê°„ë‹¨í•œ ë¡œë´‡ í•™ìŠµ
    model, env = train_simple_robot()
    env.close()
    
    # 3. í•™ìŠµëœ ë¡œë´‡ í…ŒìŠ¤íŠ¸
    test_trained_robot()
    
    # 4. ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
    advanced_robot_training()
    
    print("\nğŸ‰ PyBullet ë¡œë´‡ í•™ìŠµ ì™„ë£Œ!")
    print("ë‹¤ìŒ ë‹¨ê³„: ë” ë³µì¡í•œ í™˜ê²½ì´ë‚˜ Isaac Labìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")